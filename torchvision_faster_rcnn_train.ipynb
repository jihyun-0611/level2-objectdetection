{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "# faster rcnn modelÏù¥ Ìè¨Ìï®Îêú library\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset ÏÉùÏÑ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation = '../../dataset/train.json'\n",
    "coco = COCO(annotation)\n",
    "image_id = coco.getImgIds(imgIds=0)\n",
    "image_info = coco.loadImgs(image_id)[0]\n",
    "ann_ids = coco.getAnnIds(imgIds=image_info['id'])\n",
    "anns = coco.loadAnns(ann_ids)\n",
    "labels = np.array([x['category_id']+1 for x in anns]) \n",
    "labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    '''\n",
    "      data_dir: dataÍ∞Ä Ï°¥Ïû¨ÌïòÎäî Ìè¥Îçî Í≤ΩÎ°ú\n",
    "      transforms: data transform (resize, crop, Totensor, etc,,,)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, annotation, data_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # coco annotation Î∂àÎü¨Ïò§Í∏∞ (coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "        self.predictions = {\n",
    "            \"images\": self.coco.dataset[\"images\"].copy(),\n",
    "            \"categories\": self.coco.dataset[\"categories\"].copy(),\n",
    "            \"annotations\": None\n",
    "        }\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        boxes = np.array([x['bbox'] for x in anns])\n",
    "\n",
    "        # boxex (x_min, y_min, x_max, y_max)\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        \n",
    "        # torchvision faster_rcnnÏùÄ label=0ÏùÑ backgroundÎ°ú Ï∑®Í∏â\n",
    "        # class_idÎ•º 1~10ÏúºÎ°ú ÏàòÏ†ï \n",
    "        labels = np.array([x['category_id']+1 for x in anns]) \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        areas = np.array([x['area'] for x in anns])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "                                \n",
    "        is_crowds = np.array([x['iscrowd'] for x in anns])\n",
    "        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n",
    "\n",
    "        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([index]), 'area': areas,\n",
    "                  'iscrowd': is_crowds}\n",
    "\n",
    "        # transform\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            target['boxes'] = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
    "\n",
    "        return image, target, image_id\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(1024, 1024),\n",
    "        A.Flip(p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(num_epochs, train_data_loader, optimizer, model, device):\n",
    "    best_loss = 1000\n",
    "    total_loss_hist = Averager()\n",
    "    cls_loss_hist = Averager()\n",
    "    box_loss_hist = Averager()\n",
    "    rpn_cls_loss_hist = Averager()\n",
    "    rpn_box_loss_hist = Averager()\n",
    "    \n",
    "    checkpoint_dir = './checkpoints'\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss_hist.reset()\n",
    "        cls_loss_hist.reset()\n",
    "        box_loss_hist.reset()\n",
    "        rpn_cls_loss_hist.reset()\n",
    "        rpn_box_loss_hist.reset()\n",
    "\n",
    "        for images, targets, image_ids in tqdm(train_data_loader):\n",
    "\n",
    "            # gpu Í≥ÑÏÇ∞ÏùÑ ÏúÑÌï¥ image.to(device)\n",
    "            images = list(image.float().to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # calculate loss\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "\n",
    "            total_loss_hist.send(loss_value)\n",
    "            \n",
    "            cls_loss_hist.send(loss_dict['loss_classifier'].item())\n",
    "            box_loss_hist.send(loss_dict['loss_box_reg'].item())\n",
    "            rpn_cls_loss_hist.send(loss_dict['loss_objectness'].item())\n",
    "            rpn_box_loss_hist.send(loss_dict['loss_rpn_box_reg'].item())\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        epoch_loss = total_loss_hist.value\n",
    "        loss_cls = cls_loss_hist.value\n",
    "        loss_box_reg = box_loss_hist.value\n",
    "        loss_rpn_cls = rpn_cls_loss_hist.value\n",
    "        loss_rpn_loc = rpn_box_loss_hist.value\n",
    "        \n",
    "        print(f\"Epoch #{epoch+1} loss: {epoch_loss}\")\n",
    "        \n",
    "        mlflow.log_metrics({\n",
    "            \"total_loss\": epoch_loss,\n",
    "            \"loss_cls\": loss_cls,\n",
    "            \"loss_box_reg\" : loss_box_reg,\n",
    "            \"loss_rpn_cls\" : loss_rpn_cls,\n",
    "            \"loss_rpn_loc\" : loss_rpn_loc,\n",
    "            }, step=epoch)\n",
    "        \n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            \n",
    "            save_path = os.path.join(checkpoint_dir, f'faster_rcnn_torchvision_{epoch+1}.pth')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            \n",
    "            # MLflow Î™®Îç∏ Ï†ÄÏû•\n",
    "            mlflow.pytorch.log_model(model, f'faster_rcnn_torchvision_{epoch+1}.pth')\n",
    "            mlflow.log_artifact(save_path)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    mlflow.set_tracking_uri(\"http://localhost:30280\")\n",
    "    \n",
    "    experiment_name = \"Faster_RCNN_COCO\"\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    try:\n",
    "        with mlflow.start_run():\n",
    "            # ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï\n",
    "            num_epochs = 5\n",
    "            batch_size = 16\n",
    "            learning_rate = 0.005\n",
    "            momentum = 0.9\n",
    "            weight_decay = 0.0005\n",
    "        \n",
    "            # Îç∞Ïù¥ÌÑ∞ÏÖã Î∂àÎü¨Ïò§Í∏∞\n",
    "            annotation = '/data/ephemeral/home/workspace/dataset/train.json' # annotation Í≤ΩÎ°ú\n",
    "            data_dir = '/data/ephemeral/home/workspace/dataset' # data_dir Í≤ΩÎ°ú\n",
    "            train_dataset = CustomDataset(annotation, data_dir, get_train_transform()) \n",
    "            train_data_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "                collate_fn=collate_fn\n",
    "            )\n",
    "            \n",
    "            device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "            print(f\"Using device: {device}\")\n",
    "            \n",
    "            # torchvision model Î∂àÎü¨Ïò§Í∏∞\n",
    "            model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "            num_classes = 11 # class Í∞úÏàò= 10 + background\n",
    "            # get number of input features for the classifier\n",
    "            in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "            model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "            model.to(device)\n",
    "            \n",
    "            # optimizer \n",
    "            params = [p for p in model.parameters() if p.requires_grad]\n",
    "            optimizer = torch.optim.SGD(params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "        \n",
    "            # ML flowÏóê ÌïòÏù¥Ìçº ÌååÎùºÎØ∏ÌÑ∞ Î°úÍπÖ\n",
    "            mlflow.log_params({\n",
    "                    \"num_epochs\": num_epochs,\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"momentum\": momentum,\n",
    "                    \"weight_decay\": weight_decay\n",
    "            })\n",
    "            \n",
    "            # training\n",
    "            trained_model = train_fn(num_epochs, train_data_loader, optimizer, model, device)\n",
    "            \n",
    "            # ÏµúÏ¢Ö Í≤∞Í≥º Î°úÍπÖ\n",
    "            final_dir = './final_models'\n",
    "            if not os.path.exists(final_dir):\n",
    "                os.makedirs(final_dir)\n",
    "            final_model_path = os.path.join(final_dir, \"final_model.pth\")\n",
    "            torch.save(trained_model.state_dict(), final_model_path)\n",
    "            mlflow.log_artifact(final_model_path)\n",
    "            \n",
    "            # model signature \n",
    "            trained_model.eval()\n",
    "            \n",
    "            sample_input = next(iter(train_data_loader))[0][0].unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                sample_output = trained_model(sample_input)\n",
    "                \n",
    "            # ÏãúÍ∑∏ÎãàÏ≤ò ÏÉùÏÑ±\n",
    "            input_sample = sample_input.cpu().numpy()\n",
    "            output_sample = {k: v.cpu().numpy() for k, v in sample_output[0].items()}\n",
    "            signature = infer_signature(input_sample, output_sample)\n",
    "            \n",
    "            # ÏµúÏ¢Ö Î™®Îç∏ Î°úÍπÖ \n",
    "            mlflow.pytorch.log_model(trained_model, \"final_model\", signature=signature)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occured: {e}\")\n",
    "        mlflow.log_param(\"error\", str(e))\n",
    "    finally:\n",
    "        mlflow.end_run()    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.22s)\n",
      "creating index...\n",
      "index created!\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306/306 [04:32<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 loss: 0.6454067307652211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/10 03:35:36 WARNING mlflow.utils.requirements_utils: Found torch version (1.12.1+cu116) contains a local version label (+cu116). MLflow logged a pip requirement for this package as 'torch==1.12.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/10/10 03:35:41 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.13.1+cu116) contains a local version label (+cu116). MLflow logged a pip requirement for this package as 'torchvision==0.13.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/10/10 03:35:41 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306/306 [04:30<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2 loss: 0.5096143619492163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/10 03:40:15 WARNING mlflow.utils.requirements_utils: Found torch version (1.12.1+cu116) contains a local version label (+cu116). MLflow logged a pip requirement for this package as 'torch==1.12.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/10/10 03:40:19 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.13.1+cu116) contains a local version label (+cu116). MLflow logged a pip requirement for this package as 'torchvision==0.13.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/10/10 03:40:19 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306/306 [04:31<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3 loss: 0.47500742285274994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/10 03:44:54 WARNING mlflow.utils.requirements_utils: Found torch version (1.12.1+cu116) contains a local version label (+cu116). MLflow logged a pip requirement for this package as 'torch==1.12.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/10/10 03:44:59 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.13.1+cu116) contains a local version label (+cu116). MLflow logged a pip requirement for this package as 'torchvision==0.13.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/10/10 03:44:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306/306 [04:31<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4 loss: 0.4526448973448448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/10 03:49:34 WARNING mlflow.utils.requirements_utils: Found torch version (1.12.1+cu116) contains a local version label (+cu116). MLflow logged a pip requirement for this package as 'torch==1.12.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/10/10 03:49:38 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.13.1+cu116) contains a local version label (+cu116). MLflow logged a pip requirement for this package as 'torchvision==0.13.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/10/10 03:49:38 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306/306 [04:31<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5 loss: 0.4319207645417039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/10 03:54:13 WARNING mlflow.utils.requirements_utils: Found torch version (1.12.1+cu116) contains a local version label (+cu116). MLflow logged a pip requirement for this package as 'torch==1.12.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/10/10 03:54:17 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.13.1+cu116) contains a local version label (+cu116). MLflow logged a pip requirement for this package as 'torchvision==0.13.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/10/10 03:54:17 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/10/10 03:54:22 WARNING mlflow.utils.requirements_utils: Found torch version (1.12.1+cu116) contains a local version label (+cu116). MLflow logged a pip requirement for this package as 'torch==1.12.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/10/10 03:54:26 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.13.1+cu116) contains a local version label (+cu116). MLflow logged a pip requirement for this package as 'torchvision==0.13.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/10/10 03:54:27 INFO mlflow.tracking._tracking_service.client: üèÉ View run delightful-robin-384 at: http://localhost:30280/#/experiments/383093648340972743/runs/1cb7a64f7c534a6597d2dec47d3ab095.\n",
      "2024/10/10 03:54:27 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://localhost:30280/#/experiments/383093648340972743.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
